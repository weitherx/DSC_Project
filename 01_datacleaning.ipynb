{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036514b5",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708287df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', None)     # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Disable line wrapping\n",
    "pd.set_option('display.max_colwidth', None) # Show long text fully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe6935",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c557c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res = pd.read_csv('一般訂單.csv')\n",
    "df_un = pd.read_csv(r'C:\\Users\\USER\\OneDrive\\Documents\\DSC_Project\\20250517_資料集\\customization.csv')\n",
    "df_raw = df_un\n",
    "folder_name = 'costumization_clean'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e46b80",
   "metadata": {},
   "source": [
    "### Define Fnution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2a0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset except for numeric are all strings, so we have to convert into python objects.\n",
    "\n",
    "# Checking for missing commas in the list of dictionaries\n",
    "def fix_missing_commas(raw_string):\n",
    "    \"\"\"\n",
    "    Insert commas between dictionaries in lists, e.g., [{...}{...}] -> [{...}, {...}]\n",
    "    \"\"\"\n",
    "    fixed_string = re.sub(r'\\}[\\s]*\\{', '}, {', raw_string)\n",
    "    return fixed_string\n",
    "\n",
    "def parse_json_like(val):\n",
    "    \"\"\"\n",
    "    將 string 讀成 Python 物件\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(val, str):\n",
    "            val = fix_missing_commas(val)\n",
    "            fixed = re.sub(r'\\}[\\s]*\\{', '}, {', val)\n",
    "            #處理時間資料\n",
    "            fixed_time = re.sub(r'Timestamp\\((.*?)\\)', r'\\1', fixed)\n",
    "            #處理array型式\n",
    "            cleaned = re.sub(r'array\\((\\[.*?\\])\\s*,\\s*dtype=.*?\\)', r'\\1', fixed_time)\n",
    "\n",
    "            return ast.literal_eval(cleaned)\n",
    "        else:\n",
    "            return val\n",
    "    except Exception as e:\n",
    "        #print(str_val)\n",
    "        #print(f\"Error parsing services : {e}\")\n",
    "        return val\n",
    "    \n",
    "def extend_dict_like(df, col_name, converted=False, rename=True):\n",
    "    \"\"\"\n",
    "    Extend the dictionary-like column into separate columns.\n",
    "    \"\"\"\n",
    "    if not converted:\n",
    "        df_parsed = df[col_name].apply(parse_json_like)\n",
    "    else:\n",
    "        df_parsed = df[col_name]\n",
    "    df_expanded = df_parsed.apply(lambda x: pd.Series(x))\n",
    "\n",
    "    try:\n",
    "        df_expanded.drop([0], axis=1, inplace=True) # Drops any column named 0 (likely a bug fix for unexpected data).\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    if rename:\n",
    "        df_expanded.columns = [col_name + \"_\" + str(key) for key in df_expanded.columns]\n",
    "    return df_expanded\n",
    "\n",
    "def extract_dict_list_like(df, col_name, id_col):\n",
    "    \"\"\"\n",
    "    Convert a column of lists of dictionaries into a separate table.\n",
    "    \"\"\"\n",
    "    df_current = df[[id_col, col_name]].rename(columns = {id_col: 'order_id'})\n",
    "    df_exploded = df_current.explode(col_name)\n",
    "    detailed_df = extend_dict_like(df_exploded, col_name, converted = True, rename = False)\n",
    "\n",
    "    try:\n",
    "        detailed_df.drop([0], axis=1, inplace=True) # Drops any column named 0 (likely a bug fix for unexpected data).\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    df_final = pd.concat([df_exploded['order_id'], detailed_df] ,axis=1)\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4561d",
   "metadata": {},
   "source": [
    "### Main Table Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e340ba1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 1\n",
      "Isolating a new subtable for column: items\n",
      "Problem processing items ... added to error_columns list\n",
      "No more columns to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15252\\3560262479.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_parsed = df_raw.applymap(parse_json_like)  # Parse all cells\n"
     ]
    }
   ],
   "source": [
    "df_parsed = df_raw.applymap(parse_json_like)  # Parse all cells\n",
    "df = df_parsed.copy()\n",
    "\n",
    "error_columns = []\n",
    "subDF_dict = {}\n",
    "\n",
    "loops = 0\n",
    "while True:\n",
    "    loops += 1\n",
    "    flag = 0\n",
    "    print(f\"Loop {loops}\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in error_columns:\n",
    "            continue\n",
    "\n",
    "        if df[col].isna().all():\n",
    "            continue\n",
    "        else:\n",
    "            sample = df[col].dropna().iloc[0]\n",
    "\n",
    "        if isinstance(sample, dict):\n",
    "            print(f\"Expanding dict : {col}\")\n",
    "            flag = 1\n",
    "            try:\n",
    "                col_expend = extend_dict_like(df, col, converted = True)\n",
    "                df = pd.concat([df, col_expend], axis=1)\n",
    "                df.drop([col], axis=1, inplace=True)\n",
    "            except:\n",
    "                print(f\"Problem processing {col} ... added to error_columns list\")\n",
    "                error_columns.append(col)\n",
    "        elif isinstance(sample, list):\n",
    "            print(f\"Isolating a new subtable for column: {col}\")\n",
    "            try:\n",
    "                expanded_df = extract_dict_list_like(df, col, \"_id_oid\")\n",
    "                subDF_dict[col] = expanded_df\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "            except:\n",
    "                print(f\"Problem processing {col} ... added to error_columns list\")\n",
    "                error_columns.append(col)\n",
    "    \n",
    "    if flag == 0:\n",
    "        print(\"No more columns to process.\")\n",
    "        break\n",
    "\n",
    "# Save main table\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "df.to_csv(f\"{folder_name}/main.csv\", index=False)\n",
    "\n",
    "# Save sub-tables\n",
    "for sub_table, sub_df in subDF_dict.items():\n",
    "    sub_df.to_csv(f\"{folder_name}/{sub_table}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25dc18",
   "metadata": {},
   "source": [
    "### SubTable Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a24aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_columns = []\n",
    "for subt_table in subDF_dict.keys():\n",
    "    print(\"\\n=====\\n\")\n",
    "    print(f\"Cleaning sub_table {sub_table}\")\n",
    "    df_cleaned = subDF_dict[sub_table]\n",
    "    df_cleaned.dropna(how='all', inplace=True) # Drop rows where all elements are NaN\n",
    "    cols = df_cleaned.columns\n",
    "\n",
    "    loops = 0\n",
    "    while True:\n",
    "            loops += 1\n",
    "            flag = 0\n",
    "            print(f\"Loop : {loops}\")\n",
    "            for col in df_cleaned.columns:\n",
    "                if df_cleaned[col].isna().all():\n",
    "                    continue\n",
    "                else:\n",
    "                    sample = df_cleaned[col].dropna().iloc[0]\n",
    "                if type(sample) == type(dict()):\n",
    "                    print(f\"Processing dict : {col}\")\n",
    "                    flag = 1\n",
    "                    try:\n",
    "                        col_expanded = extend_dict_like(df_cleaned, col, converted=True)\n",
    "                        df_cleaned = pd.concat([df_cleaned, col_expanded], axis=1)\n",
    "                        df_cleaned.drop(col, axis=1, inplace=True)\n",
    "                    except:\n",
    "                        print(f\"Problem processing {col} ... added to error_columns list\")\n",
    "                        error_columns.append(col)\n",
    "            if flag == 0:\n",
    "                break\n",
    "            \n",
    "    df_cleaned.to_csv(f\"{folder_name}/{sub_table}.csv\", index=False)\n",
    "    print(f\"Subtable saved to folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96229afa",
   "metadata": {},
   "source": [
    "### Documenting the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bbfda07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料夾名稱: costumization_clean\n",
      "\n",
      "======\n",
      "\n",
      "主表: main.csv\n",
      "\n",
      "副表列表 (0):\n",
      "\n",
      "=====\n",
      "\n",
      "主表副表連結: 主表的 _id_oid 欄位對上 副表的 order_id_oid 欄位\n"
     ]
    }
   ],
   "source": [
    "print(f\"資料夾名稱: {folder_name}\")\n",
    "print(\"\\n======\\n\")\n",
    "print(\"主表: main.csv\\n\")\n",
    "print(f\"副表列表 ({len(subDF_dict.keys())}):\")\n",
    "for key in subDF_dict.keys():\n",
    "    print(f\"-- {key}.csv\")\n",
    "print(\"\\n=====\\n\")\n",
    "print(f\"主表副表連結: 主表的 _id_oid 欄位對上 副表的 order_id_oid 欄位\")\n",
    "\n",
    "with open(f\"{folder_name}\\Description.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"資料夾名稱: {folder_name}\\n\")\n",
    "    f.write(\"\\n======\\n\\n\")\n",
    "    f.write(\"主表: main.csv\\n\\n\")\n",
    "    f.write(f\"副表列表 ({len(subDF_dict.keys())}):\\n\")\n",
    "    for key in subDF_dict.keys():\n",
    "        f.write(f\"-- {key}.csv\\n\")\n",
    "    f.write(\"\\n=====\\n\\n\")\n",
    "    f.write(\"主表副表連結: 主表的 _id_oid 欄位對上 副表的 order_id_oid 欄位\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
